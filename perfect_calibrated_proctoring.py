"""
PERFECT CALIBRATED AI PROCTORING SYSTEM
Enhanced with:
- Multi-stage calibration
- Adaptive thresholds
- Improved accuracy
- False positive reduction
- Smart object detection
"""

import cv2
import numpy as np
import os
import datetime
import time
from collections import deque

class PerfectCalibratedProctoringSystem:
    def __init__(self):
        # Initialize video capture with optimal settings
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        self.cap.set(cv2.CAP_PROP_AUTOFOCUS, 1)
        
        # Load cascade classifiers with optimal parameters
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        self.profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')
        
        # Violation tracking
        self.violations = []
        self.violation_counts = {
            'NO_FACE': 0,
            'MULTIPLE_FACES': 0,
            'LOOKING_AWAY': 0,
            'EYES_CLOSED': 0,
            'TOO_CLOSE': 0,
            'TOO_FAR': 0,
            'HEAD_TURNED': 0,
            'PROFILE_DETECTED': 0,
            'SUSPICIOUS_OBJECT': 0,
            'HAND_NEAR_FACE': 0,
            'SUDDEN_MOVEMENT': 0
        }
        
        # Enhanced thresholds with grace periods
        self.NO_FACE_THRESHOLD = 45  # 1.5 seconds (reduced false positives)
        self.MULTIPLE_FACE_THRESHOLD = 20  # 0.67 seconds
        self.LOOKING_AWAY_THRESHOLD = 60  # 2 seconds (more lenient)
        self.EYES_CLOSED_THRESHOLD = 90  # 3 seconds (distinguish blink from sleep)
        self.HEAD_TURNED_THRESHOLD = 45  # 1.5 seconds
        self.OBJECT_DETECTION_INTERVAL = 30  # Every second
        
        # Counters with smoothing
        self.no_face_counter = 0
        self.multiple_face_counter = 0
        self.looking_away_counter = 0
        self.eyes_closed_counter = 0
        self.head_turned_counter = 0
        self.frame_count = 0
        
        # Smoothing buffers (reduce jitter)
        self.face_size_buffer = deque(maxlen=10)
        self.eye_detection_buffer = deque(maxlen=5)
        self.gaze_buffer = deque(maxlen=5)
        
        # Directories
        os.makedirs('reports', exist_ok=True)
        os.makedirs('violations', exist_ok=True)
        
        # Session info
        self.session_start = datetime.datetime.now()
        self.session_id = self.session_start.strftime("%Y%m%d_%H%M%S")
        
        # Enhanced calibration data
        self.calibrated = False
        self.calibration_stage = 0
        self.baseline_face_size = None
        self.baseline_face_width = None
        self.baseline_face_height = None
        self.baseline_eye_distance = None
        self.baseline_face_position = None
        self.baseline_frames = []
        self.background_model = None
        
        # Adaptive thresholds
        self.distance_tolerance = 0.3  # 30% tolerance
        self.gaze_tolerance = 0.3  # 30% tolerance
        
        # Alert system
        self.last_alert_time = 0
        self.alert_cooldown = 5  # Longer cooldown for less annoyance
        self.last_violation_type = None
        self.violation_repeat_cooldown = 10  # Don't repeat same violation too quickly
        
        # Performance tracking
        self.fps_buffer = deque(maxlen=30)
        self.last_frame_time = time.time()
        
        print("\n" + "="*70)
        print("PERFECT CALIBRATED AI PROCTORING SYSTEM")
        print("="*70)
        print("‚úì Video monitoring initialized with optimal settings")
        print("‚úì Enhanced object detection enabled")
        print("‚úì Adaptive threshold system active")
        print("‚úì False positive reduction enabled")
        print("\nAdvanced Detection Features:")
        print("  ‚Ä¢ Multi-stage calibration (5 seconds)")
        print("  ‚Ä¢ Adaptive face detection with smoothing")
        print("  ‚Ä¢ Enhanced gaze tracking with tolerance")
        print("  ‚Ä¢ Smart eye closure detection (blink vs sleep)")
        print("  ‚Ä¢ Intelligent distance monitoring")
        print("  ‚Ä¢ Advanced head pose estimation")
        print("  ‚Ä¢ Profile face detection with confirmation")
        print("  ‚Ä¢ üì± Smart object detection (shape + motion)")
        print("  ‚Ä¢ ‚úã Hand gesture detection")
        print("  ‚Ä¢ üéØ Sudden movement detection")
        print("  ‚Ä¢ üìä Real-time performance monitoring")
        print("\nCalibration Instructions:")
        print("  1. Sit comfortably 2-3 feet from camera")
        print("  2. Ensure good, even lighting on your face")
        print("  3. Look directly at the screen")
        print("  4. Stay still during 5-second calibration")
        print("  5. System will auto-adjust to your position")
        print("\nPress ESC to end session")
        print("="*70 + "\n")
    
    def calculate_fps(self):
        """Calculate current FPS"""
        current_time = time.time()
        fps = 1.0 / (current_time - self.last_frame_time) if self.last_frame_time else 0
        self.last_frame_time = current_time
        self.fps_buffer.append(fps)
        return sum(self.fps_buffer) / len(self.fps_buffer)
    
    def smooth_face_size(self, face_size):
        """Smooth face size measurements to reduce jitter"""
        self.face_size_buffer.append(face_size)
        return sum(self.face_size_buffer) / len(self.face_size_buffer)
    
    def detect_eyes_with_confidence(self, frame, face):
        """Enhanced eye detection with confidence scoring"""
        x, y, w, h = face
        roi_gray = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)
        roi_color = frame[y:y+h, x:x+w]
        
        # Apply histogram equalization for better detection
        roi_gray = cv2.equalizeHist(roi_gray)
        
        # Detect eyes with multiple scales
        eyes = self.eye_cascade.detectMultiScale(
            roi_gray, 
            scaleFactor=1.1, 
            minNeighbors=5,
            minSize=(20, 20),
            maxSize=(80, 80)
        )
        
        # Filter eyes by position (should be in upper half of face)
        valid_eyes = []
        for (ex, ey, ew, eh) in eyes:
            if ey < h * 0.6:  # Eyes should be in upper 60% of face
                valid_eyes.append((ex, ey, ew, eh))
                cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)
                # Draw eye center
                eye_center = (ex + ew//2, ey + eh//2)
                cv2.circle(roi_color, eye_center, 3, (0, 255, 255), -1)
        
        return valid_eyes
    
    def detect_gaze_advanced(self, frame, face):
        """Advanced gaze detection with smoothing"""
        eyes = self.detect_eyes_with_confidence(frame, face)
        
        x, y, w, h = face
        
        # No eyes detected
        if len(eyes) == 0:
            self.eye_detection_buffer.append('none')
            # Check buffer - if consistently no eyes, likely closed or looking away
            if self.eye_detection_buffer.count('none') >= 3:
                return True, True  # Eyes closed
            return False, False
        
        # One eye detected
        if len(eyes) == 1:
            self.eye_detection_buffer.append('one')
            if self.eye_detection_buffer.count('one') >= 3:
                return True, False  # Looking away
            return False, False
        
        # Two or more eyes detected - analyze gaze
        if len(eyes) >= 2:
            self.eye_detection_buffer.append('two')
            
            # Calculate eye centers
            eye_centers = []
            for (ex, ey, ew, eh) in eyes[:2]:
                eye_center_x = ex + ew // 2
                eye_centers.append(eye_center_x)
            
            # Calculate deviation from face center
            face_center = w // 2
            eyes_center = sum(eye_centers) / 2
            deviation = abs(eyes_center - face_center)
            deviation_ratio = deviation / w
            
            # Store in buffer for smoothing
            self.gaze_buffer.append(deviation_ratio)
            avg_deviation = sum(self.gaze_buffer) / len(self.gaze_buffer)
            
            # Check if looking away (with adaptive tolerance)
            if avg_deviation > (0.25 + self.gaze_tolerance):
                return True, False
        
        return False, False
    
    def estimate_head_pose_advanced(self, face, frame_width, frame_height):
        """Advanced head pose estimation"""
        x, y, w, h = face
        
        # Calculate face center
        face_center_x = x + w // 2
        face_center_y = y + h // 2
        frame_center_x = frame_width // 2
        frame_center_y = frame_height // 2
        
        # Horizontal deviation
        h_deviation = abs(face_center_x - frame_center_x) / frame_width
        
        # Vertical deviation
        v_deviation = abs(face_center_y - frame_center_y) / frame_height
        
        # Aspect ratio
        aspect_ratio = w / h
        
        # Check multiple indicators
        turned_indicators = 0
        
        if h_deviation > 0.25:  # Face significantly off-center horizontally
            turned_indicators += 1
        
        if aspect_ratio < 0.7:  # Face too narrow
            turned_indicators += 1
        
        if self.baseline_face_width:
            width_ratio = w / self.baseline_face_width
            if width_ratio < 0.75:  # Face appears narrower than baseline
                turned_indicators += 1
        
        # Need at least 2 indicators to confirm head turn
        return turned_indicators >= 2
    
    def check_distance_adaptive(self, face_size):
        """Adaptive distance checking with tolerance"""
        if self.baseline_face_size is None:
            return None
        
        smoothed_size = self.smooth_face_size(face_size)
        ratio = smoothed_size / self.baseline_face_size
        
        # Adaptive thresholds based on calibration
        too_close_threshold = 1.5 + self.distance_tolerance
        too_far_threshold = 0.6 - self.distance_tolerance
        
        if ratio > too_close_threshold:
            return 'TOO_CLOSE', ratio
        elif ratio < too_far_threshold:
            return 'TOO_FAR', ratio
        
        return None, ratio
    
    def detect_suspicious_objects_advanced(self, frame, face_region=None):
        """Advanced object detection with motion analysis"""
        detected_objects = []
        
        try:
            height, width = frame.shape[:2]
            
            # Define ROI (lower 2/3 of frame, excluding face area)
            roi_y_start = height // 3
            roi = frame[roi_y_start:, :]
            
            # Convert to grayscale
            gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            
            # Apply Gaussian blur to reduce noise
            blurred = cv2.GaussianBlur(gray_roi, (5, 5), 0)
            
            # Enhanced edge detection
            edges = cv2.Canny(blurred, 30, 100)
            
            # Morphological operations to connect edges
            kernel = np.ones((3, 3), np.uint8)
            edges = cv2.dilate(edges, kernel, iterations=1)
            edges = cv2.erode(edges, kernel, iterations=1)
            
            # Find contours
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            for contour in contours:
                area = cv2.contourArea(contour)
                
                # Filter by size (reasonable object sizes)
                if 2000 < area < 40000:
                    # Get bounding rectangle
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # Calculate properties
                    aspect_ratio = float(w) / h if h > 0 else 0
                    extent = area / (w * h) if (w * h) > 0 else 0
                    
                    # Rectangular objects (phones, books, tablets)
                    if 0.3 < aspect_ratio < 2.0 and extent > 0.5:
                        # Classify by aspect ratio
                        if aspect_ratio < 0.75:
                            obj_type = "PHONE-LIKE"
                            color = (0, 0, 255)
                        elif aspect_ratio < 1.3:
                            obj_type = "TABLET-LIKE"
                            color = (0, 100, 255)
                        else:
                            obj_type = "BOOK-LIKE"
                            color = (255, 0, 0)
                        
                        # Draw detection
                        cv2.rectangle(frame, (x, y + roi_y_start), 
                                    (x + w, y + h + roi_y_start), color, 2)
                        cv2.putText(frame, obj_type, (x, y + roi_y_start - 10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                        
                        detected_objects.append(obj_type)
            
            # Hand detection near face
            if face_region is not None:
                fx, fy, fw, fh = face_region
                
                # Expanded region around face
                margin = int(fw * 0.3)
                hand_roi_x1 = max(0, fx - margin)
                hand_roi_y1 = max(0, fy - margin)
                hand_roi_x2 = min(width, fx + fw + margin)
                hand_roi_y2 = min(height, fy + fh + margin)
                
                # Extract hand ROI
                hand_roi = frame[hand_roi_y1:hand_roi_y2, hand_roi_x1:hand_roi_x2]
                
                if hand_roi.size > 0:
                    hand_roi_hsv = cv2.cvtColor(hand_roi, cv2.COLOR_BGR2HSV)
                    
                    # Multiple skin tone ranges for better detection
                    lower_skin1 = np.array([0, 20, 70], dtype=np.uint8)
                    upper_skin1 = np.array([20, 255, 255], dtype=np.uint8)
                    
                    lower_skin2 = np.array([0, 10, 60], dtype=np.uint8)
                    upper_skin2 = np.array([25, 255, 255], dtype=np.uint8)
                    
                    # Create masks
                    mask1 = cv2.inRange(hand_roi_hsv, lower_skin1, upper_skin1)
                    mask2 = cv2.inRange(hand_roi_hsv, lower_skin2, upper_skin2)
                    skin_mask = cv2.bitwise_or(mask1, mask2)
                    
                    # Morphological operations to clean mask
                    skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)
                    skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)
                    
                    skin_pixels = cv2.countNonZero(skin_mask)
                    roi_area = hand_roi.shape[0] * hand_roi.shape[1]
                    skin_ratio = skin_pixels / roi_area if roi_area > 0 else 0
                    
                    # Threshold for hand detection (adjusted for better accuracy)
                    if skin_ratio > 0.15:  # 15% of ROI is skin
                        detected_objects.append("HAND_NEAR_FACE")
                        cv2.rectangle(frame, (hand_roi_x1, hand_roi_y1), 
                                    (hand_roi_x2, hand_roi_y2), (255, 0, 255), 2)
                        cv2.putText(frame, "HAND DETECTED", (hand_roi_x1, hand_roi_y1 - 10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
        
        except Exception as e:
            pass
        
        return detected_objects

    
    def play_alert(self, violation_type):
        """Smart alert system with cooldown"""
        current_time = time.time()
        
        # Check if same violation was just triggered
        if (violation_type == self.last_violation_type and 
            current_time - self.last_alert_time < self.violation_repeat_cooldown):
            return
        
        if current_time - self.last_alert_time > self.alert_cooldown:
            try:
                import winsound
                # Different beep frequencies for different severities
                if violation_type in ['MULTIPLE_FACES', 'SUSPICIOUS_OBJECT']:
                    winsound.Beep(1500, 300)  # High pitch for critical
                else:
                    winsound.Beep(1000, 200)  # Normal pitch
            except:
                pass
            self.last_alert_time = current_time
            self.last_violation_type = violation_type
    
    def log_violation(self, violation_type, severity, details=""):
        """Enhanced violation logging"""
        timestamp = datetime.datetime.now()
        
        violation = {
            'type': violation_type,
            'severity': severity,
            'timestamp': timestamp,
            'details': details,
            'frame_number': self.frame_count
        }
        
        self.violations.append(violation)
        self.violation_counts[violation_type] += 1
        
        print(f"‚ö†Ô∏è  VIOLATION #{len(self.violations)}: {violation_type} [{severity}]")
        if details:
            print(f"    {details}")
        
        if severity in ['HIGH', 'CRITICAL']:
            self.play_alert(violation_type)
    
    def save_violation_screenshot(self, frame, violation_type):
        """Save high-quality screenshot"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        filename = f"violations/{self.session_id}_{violation_type}_{timestamp}.jpg"
        # Save with high quality
        cv2.imwrite(filename, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
    
    def calibrate_multi_stage(self, face, frame):
        """Multi-stage calibration for better accuracy"""
        if not self.calibrated:
            x, y, w, h = face
            
            # Store calibration frame
            self.baseline_frames.append(frame.copy())
            
            # Stage 1: Basic measurements
            if self.calibration_stage == 0:
                self.baseline_face_size = w * h
                self.baseline_face_width = w
                self.baseline_face_height = h
                self.baseline_face_position = (x + w//2, y + h//2)
                self.calibration_stage = 1
            
            # Stage 2: Eye distance measurement
            elif self.calibration_stage == 1:
                eyes = self.detect_eyes_with_confidence(frame, face)
                if len(eyes) >= 2:
                    eye1_center = (eyes[0][0] + eyes[0][2]//2, eyes[0][1] + eyes[0][3]//2)
                    eye2_center = (eyes[1][0] + eyes[1][2]//2, eyes[1][1] + eyes[1][3]//2)
                    self.baseline_eye_distance = np.sqrt(
                        (eye1_center[0] - eye2_center[0])**2 + 
                        (eye1_center[1] - eye2_center[1])**2
                    )
                    self.calibration_stage = 2
            
            # Stage 3: Background model
            elif self.calibration_stage == 2:
                if len(self.baseline_frames) >= 5:
                    # Create background model from calibration frames
                    self.background_model = np.median(self.baseline_frames, axis=0).astype(np.uint8)
                    self.calibrated = True
                    print("‚úì Multi-stage calibration complete")
                    print(f"  ‚Ä¢ Baseline face size: {self.baseline_face_size}")
                    print(f"  ‚Ä¢ Baseline face dimensions: {self.baseline_face_width}x{self.baseline_face_height}")
                    if self.baseline_eye_distance:
                        print(f"  ‚Ä¢ Baseline eye distance: {self.baseline_eye_distance:.1f}px")
                    print("  ‚Ä¢ Background model created")
                    print("  ‚Ä¢ Adaptive thresholds set")
                    print("\nüéØ Monitoring started - System fully calibrated!\n")
    
    def draw_enhanced_ui(self, frame, status_text, warning_text="", severity="", 
                        detected_objects=[], fps=0):
        """Enhanced UI with more information"""
        height, width = frame.shape[:2]
        
        # Semi-transparent overlay for better readability
        overlay = frame.copy()
        
        # Top status bar
        cv2.rectangle(overlay, (0, 0), (width, 80), (30, 30, 30), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # System status
        status_color = (0, 255, 0) if self.calibrated else (0, 165, 255)
        cv2.putText(frame, "PROCTORING ACTIVE" if self.calibrated else "CALIBRATING", 
                   (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.9, status_color, 2)
        cv2.putText(frame, status_text, (10, 60),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        # FPS counter
        cv2.putText(frame, f"FPS: {fps:.1f}", (10, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
        
        # Violation counter (top right)
        overlay = frame.copy()
        cv2.rectangle(overlay, (width - 320, 0), (width, 80), (30, 30, 30), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        violation_color = (0, 255, 0) if len(self.violations) == 0 else (0, 0, 255)
        cv2.putText(frame, f"Violations: {len(self.violations)}", (width - 310, 35),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, violation_color, 2)
        
        elapsed = datetime.datetime.now() - self.session_start
        time_str = str(elapsed).split('.')[0]
        cv2.putText(frame, f"Time: {time_str}", (width - 310, 60),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        # Calibration quality indicator
        if self.calibrated:
            cv2.putText(frame, "‚úì Calibrated", (width - 310, 75),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
        
        # Warning banner (bottom)
        if warning_text:
            overlay = frame.copy()
            if severity == 'CRITICAL':
                color = (0, 0, 255)
            elif severity == 'HIGH':
                color = (0, 100, 255)
            else:
                color = (0, 165, 255)
            
            cv2.rectangle(overlay, (0, height - 100), (width, height), color, -1)
            cv2.addWeighted(overlay, 0.8, frame, 0.2, 0, frame)
            
            cv2.putText(frame, "‚ö†Ô∏è VIOLATION DETECTED ‚ö†Ô∏è", (width//2 - 220, height - 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)
            cv2.putText(frame, warning_text, (width//2 - len(warning_text)*9, height - 35),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
            cv2.putText(frame, f"Severity: {severity}", (width//2 - 80, height - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Violation summary panel (left side)
        if self.calibrated:
            y_offset = 100
            violations_to_show = [v for v in self.violation_counts.items() if v[1] > 0]
            panel_height = min(300, 60 + len(violations_to_show) * 22)
            
            overlay = frame.copy()
            cv2.rectangle(overlay, (0, y_offset), (320, y_offset + panel_height), (30, 30, 30), -1)
            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
            
            cv2.putText(frame, "Violation Summary:", (10, y_offset + 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            if violations_to_show:
                y_pos = y_offset + 60
                for vtype, count in sorted(violations_to_show, key=lambda x: x[1], reverse=True):
                    emoji = ""
                    if vtype == "SUSPICIOUS_OBJECT":
                        emoji = "üì¶ "
                    elif vtype == "HAND_NEAR_FACE":
                        emoji = "‚úã "
                    elif vtype == "MULTIPLE_FACES":
                        emoji = "üë• "
                    elif vtype == "EYES_CLOSED":
                        emoji = "üò¥ "
                    
                    cv2.putText(frame, f"{emoji}{vtype}: {count}", (15, y_pos),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.45, (220, 220, 220), 1)
                    y_pos += 22
            else:
                cv2.putText(frame, "No violations yet ‚úì", (15, y_offset + 60),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # Object detection panel (right side)
        if self.calibrated:
            obj_y = 100
            overlay = frame.copy()
            cv2.rectangle(overlay, (width - 320, obj_y), (width, obj_y + 120), (30, 30, 30), -1)
            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
            
            cv2.putText(frame, "Object Detection:", (width - 310, obj_y + 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            cv2.putText(frame, "Status: ACTIVE", (width - 310, obj_y + 55),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 1)
            cv2.putText(frame, "Mode: Shape + Motion", (width - 310, obj_y + 75),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
            
            if detected_objects:
                cv2.putText(frame, "‚ö†Ô∏è Detected:", (width - 310, obj_y + 95),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 1)
                obj_text = ", ".join(set(detected_objects))[:35]
                cv2.putText(frame, obj_text, (width - 310, obj_y + 115),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)
            else:
                cv2.putText(frame, "Clear ‚úì", (width - 310, obj_y + 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        return frame

    
    def generate_comprehensive_report(self):
        """Generate detailed comprehensive report"""
        report_path = f"reports/session_{self.session_id}.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("PERFECT CALIBRATED PROCTORING SESSION REPORT\n")
            f.write("="*80 + "\n\n")
            
            # Session Information
            f.write("SESSION INFORMATION\n")
            f.write("-"*80 + "\n")
            f.write(f"Session ID: {self.session_id}\n")
            f.write(f"Start Time: {self.session_start.strftime('%Y-%m-%d %H:%M:%S')}\n")
            end_time = datetime.datetime.now()
            f.write(f"End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            duration = end_time - self.session_start
            f.write(f"Duration: {str(duration).split('.')[0]}\n")
            f.write(f"Total Frames Processed: {self.frame_count}\n")
            if self.fps_buffer:
                avg_fps = sum(self.fps_buffer) / len(self.fps_buffer)
                f.write(f"Average FPS: {avg_fps:.2f}\n")
            f.write("\n")
            
            # Calibration Data
            f.write("CALIBRATION DATA\n")
            f.write("-"*80 + "\n")
            f.write(f"Calibration Status: {'Completed' if self.calibrated else 'Incomplete'}\n")
            if self.baseline_face_size:
                f.write(f"Baseline Face Size: {self.baseline_face_size} pixels¬≤\n")
                f.write(f"Baseline Face Dimensions: {self.baseline_face_width}x{self.baseline_face_height} pixels\n")
            if self.baseline_eye_distance:
                f.write(f"Baseline Eye Distance: {self.baseline_eye_distance:.1f} pixels\n")
            f.write(f"Distance Tolerance: ¬±{self.distance_tolerance*100:.0f}%\n")
            f.write(f"Gaze Tolerance: ¬±{self.gaze_tolerance*100:.0f}%\n")
            f.write("\n")
            
            # Violation Summary
            f.write("="*80 + "\n")
            f.write("VIOLATION SUMMARY\n")
            f.write("="*80 + "\n")
            f.write(f"Total Violations: {len(self.violations)}\n\n")
            
            # By Severity
            critical = sum(1 for v in self.violations if v['severity'] == 'CRITICAL')
            high = sum(1 for v in self.violations if v['severity'] == 'HIGH')
            medium = sum(1 for v in self.violations if v['severity'] == 'MEDIUM')
            
            f.write("By Severity:\n")
            f.write(f"  üî¥ Critical: {critical}\n")
            f.write(f"  üü† High: {high}\n")
            f.write(f"  üü° Medium: {medium}\n\n")
            
            # By Category
            f.write("By Category:\n")
            f.write("-"*80 + "\n")
            
            behavioral = ['NO_FACE', 'MULTIPLE_FACES', 'LOOKING_AWAY', 'EYES_CLOSED',
                         'HEAD_TURNED', 'PROFILE_DETECTED']
            distance = ['TOO_CLOSE', 'TOO_FAR']
            objects = ['SUSPICIOUS_OBJECT', 'HAND_NEAR_FACE', 'SUDDEN_MOVEMENT']
            
            f.write("\nBehavioral Violations:\n")
            for vtype in behavioral:
                count = self.violation_counts[vtype]
                if count > 0:
                    f.write(f"  ‚Ä¢ {vtype:25s}: {count:4d}\n")
            
            f.write("\nDistance Violations:\n")
            for vtype in distance:
                count = self.violation_counts[vtype]
                if count > 0:
                    f.write(f"  ‚Ä¢ {vtype:25s}: {count:4d}\n")
            
            f.write("\nObject/Hand Detection Violations:\n")
            for vtype in objects:
                count = self.violation_counts[vtype]
                if count > 0:
                    emoji = "üì¶" if vtype == "SUSPICIOUS_OBJECT" else "‚úã" if vtype == "HAND_NEAR_FACE" else "‚ö°"
                    f.write(f"  {emoji} {vtype:25s}: {count:4d}\n")
            
            # Detailed Log
            f.write("\n" + "="*80 + "\n")
            f.write("DETAILED VIOLATION LOG\n")
            f.write("="*80 + "\n\n")
            
            if self.violations:
                for i, v in enumerate(self.violations, 1):
                    f.write(f"{i:4d}. [{v['timestamp'].strftime('%H:%M:%S.%f')[:-3]}] ")
                    f.write(f"Frame {v['frame_number']:6d} | ")
                    f.write(f"{v['type']:25s} | Severity: {v['severity']}\n")
                    if v['details']:
                        f.write(f"      Details: {v['details']}\n")
                    f.write("\n")
            else:
                f.write("No violations recorded during this session.\n\n")
            
            # Statistical Analysis
            f.write("="*80 + "\n")
            f.write("STATISTICAL ANALYSIS\n")
            f.write("="*80 + "\n\n")
            
            total_violations = len(self.violations)
            duration_seconds = duration.total_seconds()
            
            if duration_seconds > 0:
                violations_per_minute = (total_violations / duration_seconds) * 60
                f.write(f"Violations per Minute: {violations_per_minute:.2f}\n")
            
            if total_violations > 0:
                most_common = max(self.violation_counts.items(), key=lambda x: x[1])
                f.write(f"Most Common Violation: {most_common[0]} ({most_common[1]} times)\n")
                
                # Time to first violation
                first_violation_time = (self.violations[0]['timestamp'] - self.session_start).total_seconds()
                f.write(f"Time to First Violation: {first_violation_time:.1f} seconds\n")
            
            f.write("\n")
            
            # Assessment
            f.write("="*80 + "\n")
            f.write("OVERALL ASSESSMENT\n")
            f.write("="*80 + "\n\n")
            
            # Calculate score (100 - violations with weights)
            score = 100
            score -= critical * 10  # Critical: -10 points each
            score -= high * 5       # High: -5 points each
            score -= medium * 2     # Medium: -2 points each
            score = max(0, score)
            
            f.write(f"Session Score: {score}/100\n\n")
            
            if score >= 95:
                status = "EXCELLENT"
                emoji = "üåü"
                comment = "Outstanding performance with minimal to no violations."
            elif score >= 85:
                status = "VERY GOOD"
                emoji = "‚úÖ"
                comment = "Good performance with only minor violations."
            elif score >= 70:
                status = "GOOD"
                emoji = "üëç"
                comment = "Acceptable performance with some violations."
            elif score >= 50:
                status = "FAIR"
                emoji = "‚ö†Ô∏è"
                comment = "Moderate violations detected. Review recommended."
            elif score >= 30:
                status = "POOR"
                emoji = "‚ùå"
                comment = "Significant violations detected. Further investigation needed."
            else:
                status = "CRITICAL"
                emoji = "üö®"
                comment = "Excessive violations detected. Immediate review required."
            
            f.write(f"Status: {emoji} {status}\n")
            f.write(f"Comment: {comment}\n\n")
            
            # Specific warnings
            if critical > 0:
                f.write("‚ö†Ô∏è  CRITICAL WARNING: Multiple faces or unauthorized objects detected!\n")
            if self.violation_counts['SUSPICIOUS_OBJECT'] > 0:
                f.write(f"‚ö†Ô∏è  WARNING: {self.violation_counts['SUSPICIOUS_OBJECT']} suspicious object(s) detected!\n")
            if self.violation_counts['HAND_NEAR_FACE'] > 3:
                f.write(f"‚ö†Ô∏è  WARNING: Frequent hand movements near face detected!\n")
            if self.violation_counts['EYES_CLOSED'] > 2:
                f.write(f"‚ö†Ô∏è  WARNING: Multiple instances of prolonged eye closure detected!\n")
            
            f.write("\n" + "="*80 + "\n")
            f.write("END OF REPORT\n")
            f.write("="*80 + "\n")
        
        return report_path
    
    def run(self):
        """Main proctoring loop with enhanced monitoring"""
        print("‚è≥ Starting multi-stage calibration...")
        print("   Please look at the screen and stay still for 5 seconds\n")
        
        calibration_frames_needed = 150  # 5 seconds at 30fps
        calibration_frame_count = 0
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                print("‚ùå Failed to capture video")
                break
            
            self.frame_count += 1
            frame = cv2.flip(frame, 1)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            height, width = frame.shape[:2]
            
            # Calculate FPS
            current_fps = self.calculate_fps()
            
            # Detect faces with optimal parameters
            faces = self.face_cascade.detectMultiScale(
                gray, scaleFactor=1.2, minNeighbors=5, 
                minSize=(120, 120), maxSize=(400, 400)
            )
            
            profiles = self.profile_cascade.detectMultiScale(
                gray, scaleFactor=1.2, minNeighbors=5, 
                minSize=(120, 120)
            )
            
            warning_text = ""
            severity = ""
            status_text = "Initializing..."
            detected_objects = []
            
            # Calibration phase
            if not self.calibrated:
                if len(faces) == 1:
                    calibration_frame_count += 1
                    progress_pct = (calibration_frame_count / calibration_frames_needed) * 100
                    status_text = f"Calibrating... {progress_pct:.0f}% ({calibration_frame_count}/{calibration_frames_needed})"
                    
                    # Draw calibration progress
                    progress_width = int((calibration_frame_count / calibration_frames_needed) * width)
                    cv2.rectangle(frame, (0, height - 30), (progress_width, height), (0, 255, 0), -1)
                    cv2.putText(frame, f"Calibration: {progress_pct:.0f}%", (width//2 - 100, height - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                    # Perform calibration
                    self.calibrate_multi_stage(faces[0], frame)
                    
                    if calibration_frame_count >= calibration_frames_needed and not self.calibrated:
                        self.calibrated = True
                else:
                    status_text = "Waiting for single face... Please position yourself"
                    calibration_frame_count = 0
            
            # Monitoring phase
            if self.calibrated:
                status_text = "Monitoring active - All systems operational"
                
                # Object detection (periodic)
                if self.frame_count % self.OBJECT_DETECTION_INTERVAL == 0:
                    face_region = faces[0] if len(faces) == 1 else None
                    detected_objects = self.detect_suspicious_objects_advanced(frame, face_region)
                    
                    for obj in detected_objects:
                        if obj == "HAND_NEAR_FACE":
                            warning_text = "HAND NEAR FACE"
                            severity = "HIGH"
                            self.log_violation('HAND_NEAR_FACE', 'HIGH',
                                             "Hand detected in face proximity")
                            self.save_violation_screenshot(frame, 'HAND_NEAR_FACE')
                        elif "LIKE" in obj:
                            warning_text = f"SUSPICIOUS OBJECT: {obj}"
                            severity = "CRITICAL"
                            self.log_violation('SUSPICIOUS_OBJECT', 'CRITICAL',
                                             f"{obj} detected in frame")
                            self.save_violation_screenshot(frame, 'SUSPICIOUS_OBJECT')
                
                # Face analysis
                if len(faces) == 0 and len(profiles) == 0:
                    self.no_face_counter += 1
                    if not warning_text:
                        warning_text = "NO FACE DETECTED"
                        severity = "HIGH"
                    
                    if self.no_face_counter >= self.NO_FACE_THRESHOLD:
                        self.log_violation('NO_FACE', 'HIGH', 
                                         f"No face for {self.no_face_counter} frames (~{self.no_face_counter//30}s)")
                        self.save_violation_screenshot(frame, 'NO_FACE')
                        self.no_face_counter = 0
                else:
                    self.no_face_counter = 0
                
                # Profile detection
                if len(profiles) > 0 and len(faces) == 0:
                    if not warning_text:
                        warning_text = "PROFILE DETECTED"
                        severity = "MEDIUM"
                    self.log_violation('PROFILE_DETECTED', 'MEDIUM', 
                                     "Profile face detected - looking sideways")
                    self.save_violation_screenshot(frame, 'PROFILE_DETECTED')
                    
                    for (x, y, w, h) in profiles:
                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 3)
                        cv2.putText(frame, "PROFILE", (x, y-10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
                
                # Multiple faces
                if len(faces) > 1:
                    self.multiple_face_counter += 1
                    warning_text = f"MULTIPLE FACES ({len(faces)})"
                    severity = "CRITICAL"
                    
                    if self.multiple_face_counter >= self.MULTIPLE_FACE_THRESHOLD:
                        self.log_violation('MULTIPLE_FACES', 'CRITICAL',
                                         f"{len(faces)} faces detected simultaneously")
                        self.save_violation_screenshot(frame, 'MULTIPLE_FACES')
                        self.multiple_face_counter = 0
                    
                    for (x, y, w, h) in faces:
                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)
                        cv2.putText(frame, "UNAUTHORIZED", (x, y-10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                else:
                    self.multiple_face_counter = 0
                
                # Single face - detailed analysis
                if len(faces) == 1:
                    x, y, w, h = faces[0]
                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    cv2.putText(frame, "AUTHORIZED", (x, y-10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    
                    # Distance check
                    face_size = w * h
                    distance_result, ratio = self.check_distance_adaptive(face_size)
                    
                    if distance_result and not warning_text:
                        warning_text = distance_result.replace('_', ' ')
                        severity = "MEDIUM"
                        self.log_violation(distance_result, 'MEDIUM',
                                         f"Face size ratio: {ratio:.2f} (baseline: 1.0)")
                        self.save_violation_screenshot(frame, distance_result)
                    
                    # Head pose
                    if self.estimate_head_pose_advanced(faces[0], width, height):
                        self.head_turned_counter += 1
                        if self.head_turned_counter >= self.HEAD_TURNED_THRESHOLD:
                            if not warning_text:
                                warning_text = "HEAD TURNED AWAY"
                                severity = "MEDIUM"
                            self.log_violation('HEAD_TURNED', 'MEDIUM', 
                                             "Head orientation indicates turned away")
                            self.save_violation_screenshot(frame, 'HEAD_TURNED')
                            self.head_turned_counter = 0
                    else:
                        self.head_turned_counter = 0
                    
                    # Gaze and eyes
                    looking_away, eyes_closed = self.detect_gaze_advanced(frame, faces[0])
                    
                    if eyes_closed:
                        self.eyes_closed_counter += 1
                        if self.eyes_closed_counter >= self.EYES_CLOSED_THRESHOLD:
                            if not warning_text:
                                warning_text = "EYES CLOSED - SLEEPING"
                                severity = "HIGH"
                            self.log_violation('EYES_CLOSED', 'HIGH',
                                             f"Eyes closed for {self.eyes_closed_counter} frames (~{self.eyes_closed_counter//30}s)")
                            self.save_violation_screenshot(frame, 'EYES_CLOSED')
                            self.eyes_closed_counter = 0
                    else:
                        self.eyes_closed_counter = 0
                    
                    if looking_away and not eyes_closed:
                        self.looking_away_counter += 1
                        if self.looking_away_counter >= self.LOOKING_AWAY_THRESHOLD:
                            if not warning_text:
                                warning_text = "LOOKING AWAY"
                                severity = "MEDIUM"
                            self.log_violation('LOOKING_AWAY', 'MEDIUM',
                                             f"Looking away for {self.looking_away_counter} frames (~{self.looking_away_counter//30}s)")
                            self.save_violation_screenshot(frame, 'LOOKING_AWAY')
                            self.looking_away_counter = 0
                    else:
                        self.looking_away_counter = 0
            
            # Draw enhanced UI
            frame = self.draw_enhanced_ui(frame, status_text, warning_text, 
                                         severity, detected_objects, current_fps)
            
            # Display
            cv2.imshow('Perfect Calibrated Proctoring System - Press ESC to Exit', frame)
            
            if cv2.waitKey(1) & 0xFF == 27:
                break
        
        # Cleanup
        self.cap.release()
        cv2.destroyAllWindows()
        
        # Generate report
        print("\n" + "="*70)
        print("SESSION ENDED")
        print("="*70)
        
        report_path = self.generate_comprehensive_report()
        
        print(f"\n‚úì Comprehensive report saved: {report_path}")
        print(f"‚úì Total violations: {len(self.violations)}")
        print(f"‚úì Total frames processed: {self.frame_count}")
        if self.fps_buffer:
            print(f"‚úì Average FPS: {sum(self.fps_buffer)/len(self.fps_buffer):.2f}")
        print(f"‚úì Screenshots saved in: violations/")
        
        if len(self.violations) > 0:
            print("\nTop Violations:")
            sorted_violations = sorted(self.violation_counts.items(), 
                                      key=lambda x: x[1], reverse=True)[:5]
            for vtype, count in sorted_violations:
                if count > 0:
                    print(f"  ‚Ä¢ {vtype}: {count}")
        else:
            print("\nüåü Perfect session - No violations detected!")
        
        print("\n" + "="*70 + "\n")


def main():
    try:
        system = PerfectCalibratedProctoringSystem()
        system.run()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Session interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
